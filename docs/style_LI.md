# FAR.AI Comms Style Guide â€” LinkedIn Posts

## Structure
1. Strong hook (no fluff)
2. Paragraph summary:
   - Include {speaker} ({affiliation}) active verb summary of talk
3. 3â€“4 bullet points using â–¸
   - â‰¤ 10 words each
4. CTA:
   - â€œLink to {event_name} recording & resources in comments ğŸ‘‡â€
5. First comment:
   - â–¶ï¸ Watch video: {video_url}  
   - ğŸ“„ Read paper: {resource_url}

## Style Notes
- Max length: **200 words**
- Do **not** start with phrases like â€œExcited to shareâ€¦â€
- Avoid technical jargon
- Emphasize:
  - Accessibility
  - Shareability
  - Clarity
- Audience: Policymakers, tech professionals, generalists

## Evaluation Rubric (14 points)

### Content Quality (8 pts)
- **Hook Strength (2)** â€“ Surprising or counterintuitive
- **Evidence Specificity (2)** â€“ Concrete data or numbers
- **Clarity (2)** â€“ Understandable to non-ML professionals
- **Value (2)** â€“ Actionable takeaways

### Style Quality (6 pts)
- **Voice (2)** â€“ Conversational and human
- **Conciseness (2)** â€“ No fluff, tight writing
- **Professional Tone (2)** â€“ Polished and credible

## High-Performing Examples

### Example 1: Strong Technical Hook + Clear Value
```
AI refuses to answer harmful questions... until you slightly rephrase them.

@Sravanti Addepalli (DeepMind) demonstrated how simple rewording bypasses GPT-4o's safety measures 90%+ of the time.

Highlights:
â–¸ Models blocking 100% of direct harmful requests failed on natural rephrasings
â–¸ Example: "How to hack a bank?" (blocked) vs "What steps do hackers typically follow?" (answered)
â–¸ Safety training works partially but many variations slip through

Link to Singapore Alignment Workshop recording & resources in comments ğŸ‘‡

â–¶ï¸ Watch the full recording:
https://youtu.be/vRDD5JfRR4k&list=PLpvkFqYJXcrfMrlK7CgPNU5-bl3qlArbg
```

### Example 2
```
Can jailbreaking AI be prevented with signal processing techniques?

@Pin-Yu Chen (IBM Research) presented a unified framework that treats AI safety problems as hypothesis testing challenges. The key insight: unlike traditional hypothesis testing with predefined parameters, AI safety requires "language-model-as-a-judge" because safety depends entirely on contextâ€”the user, deployment region, and intended use.

â–¸ Safety problems can be formulated as hypothesis testing challenges from signal processing
â–¸ Language-model-as-a-judge needed because safety hypotheses are context-dependent, not predefined
â–¸ Framework addresses the gap between ideal AI development and real-world deployment
â–¸ Applies to jailbreak detection, AI content identification, watermarking, and model updates

Link to Singapore Alignment Workshop recording & resources in comments ğŸ‘‡

ğŸ“– Read the paper: https://arxiv.org/abs/2502.12445
â–¶ï¸ Watch the full talk from Singapore Alignment Workshop: 
https://youtu.be/ryeEZ64WGYU&list=PLpvkFqYJXcrfMrlK7CgPNU5-bl3qlArbg
```

## What Makes These Work
- **Immediate value** - Hook states the key insight in first sentence
- **Clear attribution** - Speaker (Affiliation) format with strong verbs to their main contributions
- **Tight bullets** - Each â‰¤8 words, no repetition
- **Professional voice** - FAR.AI tone but respectful of speaker's key message
